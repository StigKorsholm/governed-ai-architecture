# ADR-019: Canonical Extraction Schema for Document Intelligence

> **Status:** üü° Proposed  
> **Date:** 2026-02-06  
> **Triggered By:** ADR-001 (consequence: "Graph requires structured input from document extraction")  
> **Triggers:** Validators V-20+, Reconciliation implementation

---

## Summary

* **Decision:** All document extractors ‚Äî regardless of tool or approach ‚Äî output a common canonical schema (Legacy System Template).
* **Chosen approach:** Define a single YAML/JSON schema that captures system metadata, screens, business objects, capabilities, evidence, and quality metrics. Validators operate on this schema, not on tool-specific outputs.
* **Why:** Without a common schema, cross-approach comparison is impossible, validators must understand every extraction format, and ensemble extraction cannot work.
* **Scope:** This ADR covers the schema contract and its role in the pipeline. Tool selection, approach configuration, and implementation details (conflict resolution, cost optimization) are out of scope.

---

## Context

Document extraction is inherently probabilistic. Different approaches have different strengths:

| Approach | Characteristic | Trade-off |
|----------|----------------|-----------|
| Structured extraction | Precise on tables, lists, indexes | Misses semantic context |
| Semantic extraction | Understands meaning, infers intent | May hallucinate structure |
| Visual extraction | Captures layout, images, diagrams | Expensive, slower |
| Rule-based extraction | Deterministic, fast, repeatable | Brittle to format changes |

No single approach is reliable enough for regulated environments.

The solution is **ensemble extraction**: run multiple approaches, compare outputs, flag disagreements, and reconcile to consensus.

But ensemble extraction requires a common language. If each approach outputs a different format, comparison becomes an N√óM integration problem.

**Tool choice is implementation.** Whether you use Docling, Azure Document Intelligence, AWS Textract, LlamaParse, marker-pdf, or custom scripts ‚Äî the canonical schema is the same. What matters is that you vary your *approach*, not your *vendor*.

---

## Decision Drivers

1. **Approach-agnostic validation** ‚Äî Validators should not know or care which tool or approach produced the output
2. **Cross-approach comparison** ‚Äî Must detect when approaches agree or disagree on the same entity
3. **Completeness measurement** ‚Äî Must calculate coverage against declared inventory (TOC, screen indexes)
4. **Evidence provenance** ‚Äî Must trace every extracted entity to source page and text
5. **Graph compatibility** ‚Äî Output must be transformable to graph nodes/edges without loss
6. **Extensibility** ‚Äî New entity types (e.g., BatchJob) should not break existing extractors

---

## Considered Options

### Option 1 ‚Äî Tool-specific schemas

**Description:** Each tool/approach defines its own output format. Downstream consumers adapt.

**Pros:**
- Simple for each extractor developer
- No coordination overhead

**Cons:**
- Validators must understand every format
- Cross-approach comparison requires custom mapping per pair
- Adding new tool requires updating all consumers
- Completeness metrics become inconsistent

### Option 2 ‚Äî Graph-native output

**Description:** Extractors output Cypher statements or graph mutations directly.

**Pros:**
- No intermediate format
- Direct path to system of record

**Cons:**
- Extractors become tightly coupled to graph schema
- No staging area for validation before commit
- Difficult to compare two extraction runs
- Violates ADR-006 (LLM outputs as proposals, not commits)

### Option 3 ‚Äî Canonical extraction schema ‚úÖ **Chosen**

**Description:** Define a single schema (Legacy System Template) that all extractors must produce. Validators and reconciliation logic operate on this schema. Graph commit happens after validation.

**Pros:**
- Clear interface contract
- Validators are approach-agnostic
- Cross-approach comparison is trivial (same keys, same structure)
- Staging area enables human review before graph commit
- New tools/approaches plug in without downstream changes

**Cons:**
- Schema design requires upfront investment
- Schema evolution must be managed carefully
- Some approaches may leave fields empty (partial extraction)

---

## Decision

We will define a **Legacy System Template** schema with these principles:

### 1. Schema is the interface contract

Extractors produce it. Validators consume it. Nothing else.

```
[Approach A] ‚îÄ‚îÄ‚îê
[Approach B] ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚ñ∫ [Legacy System Template] ‚îÄ‚îÄ‚ñ∫ [Validators] ‚îÄ‚îÄ‚ñ∫ [Graph]
[Approach C] ‚îÄ‚îÄ‚îò
```

Tool selection happens inside each approach. The schema boundary is sacrosanct.

### 2. Schema captures what we need for governance

| Section | Purpose |
|---------|---------|
| `extraction_meta` | Who extracted, when, which approach, with what confidence |
| `system` | System identifier, name, domain |
| `declared_inventory` | What the document claims to contain (TOC, indexes) |
| `screens` | Extracted screens with CRUD type and evidence |
| `business_objects` | Grouped entities with CRUD coverage |
| `capabilities` | Candidate capabilities with BIAN mapping |
| `figures` | Screenshots, diagrams with descriptions |
| `unstructured_content` | Sections that didn't map to entities |
| `quality_metrics` | Coverage ratio, confidence distribution |

### 3. Every entity has evidence

No entity without provenance:

```yaml
screens:
  - id: "1.1"
    name: "Foresp√∏rgsel korrespondent"
    evidence:
      page: 27
      confidence: 0.95
      source_text: "1.1 Foresp√∏rgsel korrespondent"
```

### 4. Schema enables completeness calculation

```yaml
declared_inventory:
  screens_declared: 34
  source_pages: [27, 28]

quality_metrics:
  screens_extracted: 34
  coverage_ratio: 1.0  # extracted / declared
```

### 5. Schema is a proposal, not truth

The template is staging. Graph commit happens only after:
- Validators pass (or WARN with human override)
- Human checkpoint approves

This maintains consistency with ADR-001 (graph as truth) and ADR-006 (LLM outputs as proposals).

### 6. Approach diversity, not tool diversity

The value of ensemble extraction comes from **varying the approach**, not the vendor:

| Approach type | What it catches | What it misses |
|---------------|-----------------|----------------|
| Structured | Explicit tables, numbered lists | Implied relationships |
| Semantic | Intent, context, meaning | May invent structure |
| Visual | Layout, diagrams, spatial relationships | Text in images (depends on OCR) |
| Rule-based | Known patterns, conventions | Novel formats |

Running the same tool with the same settings twice adds no information. Running different approaches ‚Äî even with the same underlying tool ‚Äî creates the variation needed for meaningful comparison.

---

## Decision Rationale

The fundamental problem is: **how do you know extraction is complete?**

Without a canonical schema:
- Each approach reports confidence differently
- "90% confidence" from Approach A is not comparable to Approach B
- Completeness is a guess, not a measurement

With a canonical schema:
- All approaches fill the same `declared_inventory` and `quality_metrics`
- Validators compute coverage: `extracted / declared`
- Cross-approach comparison reveals gaps: "Approach A found 34, Approach B found 31"
- Human reviews the delta, not the entire extraction

This transforms document extraction from **"trust the model"** to **"verify the output."**

---

## Consequences

### Architectural Implications

| Consequence | Impact |
|-------------|--------|
| Validators V-20+ operate on this schema | Must define validator specs |
| Reconciliation logic compares template instances | Implementation detail |
| Graph ingestion transforms template ‚Üí nodes/edges | Covered by existing graph pipeline |
| Tool selection is decoupled from architecture | Teams can swap tools without ADR changes |

### Positive Consequences

- Approach-agnostic validation ‚Äî add new tools without validator changes
- Measurable completeness ‚Äî coverage ratio is a number, not a feeling
- Cross-approach consensus ‚Äî ensemble extraction becomes possible
- Clear staging area ‚Äî human review before graph commit
- Audit trail ‚Äî template instances are artifacts (ADR-003)
- Tool flexibility ‚Äî swap Docling for Azure DI tomorrow, schema unchanged

### Negative Consequences

- Upfront schema design required before new extractors
- Schema evolution needs versioning discipline
- Partial extractions require policy (accept or reject?)

### What Breaks Without This Decision

| Failure Mode | Consequence |
|--------------|-------------|
| Validators become tool-specific | N validators √ó M tools = combinatorial complexity |
| Completeness is unmeasurable | "Did we get everything?" becomes unanswerable |
| Ensemble extraction impossible | No common basis for comparison |
| Evidence provenance inconsistent | Some approaches track pages, others don't |
| Graph ingestion fragile | Every tool format requires custom mapping |
| Vendor lock-in | Changing tools means rewriting validators |

---

## Notes / Out of Scope

- **Schema definition** (YAML/JSON Schema) is implementation ‚Äî will live in MIP repo under `/schemas/`
- **Tool selection** (Docling, Azure DI, etc.) is implementation
- **Approach configuration** (OCR settings, pipeline modes) is implementation
- **Conflict resolution strategy** (majority vote, confidence-weighted) is implementation
- **Cost optimization** (cheap approaches first) is operational
- **Feedback loops** (corrections improve future extractions) is covered by ADR-017/018

---

## References

- **ADR-001:** Graph as System of Record
- **ADR-006:** LLM Outputs as Proposals
- **ADR-010:** Validator-Backed Confidence
- **ADR-003:** Artifacts in Filesystem with Run-Scoped Paths
